{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Gaussian Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from lightgbmlss.model import LightGBMLSS\n",
    "from lightgbmlss.utils import create_mv_dataset\n",
    "from lightgbmlss.distributions.Gaussian import MultivariateGaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We generate data with a single input and three outputs. The input and outputs have a smooth, non-monotonic relationship and correlated noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "x = np.linspace(0, 10, n_samples)\n",
    "\n",
    "# Create correlated noise with increasing magnitude\n",
    "def scale_cov_matrix(x, base_cov, scale_factor=0.1):\n",
    "    return base_cov * (1 + scale_factor * x)\n",
    "\n",
    "\n",
    "cov_matrix = np.array([[0.25, 0.15, 0.1], [0.15, 0.49, 0.2], [0.1, 0.2, 0.36]])\n",
    "\n",
    "correlated_noise = np.array(\n",
    "    [\n",
    "        np.random.multivariate_normal(\n",
    "            mean=[0, 0, 0], cov=scale_cov_matrix(xi, cov_matrix)\n",
    "        )\n",
    "        for xi in x\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate three dependent variables with smooth, non-monotonic relationships and correlated noise\n",
    "y1_ = 2 * np.sin(x) + 0.5 * x\n",
    "y1 = y1_ + correlated_noise[:, 0]\n",
    "y2_ = 3 * np.cos(0.5 * x) + 0.3 * x**2\n",
    "y2 = y2_ + correlated_noise[:, 1]\n",
    "y3_ = 1.5 * np.sin(0.7 * x) * np.cos(0.3 * x) + 0.2 * x\n",
    "y3 = y3_ + correlated_noise[:, 2]\n",
    "\n",
    "# Combine the data\n",
    "data = np.column_stack((x, y1, y2, y3))\n",
    "real_data = np.column_stack((x, y1_, y2_, y3_))\n",
    "\n",
    "# Plot the relationships\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "fig.suptitle(\"Relationships between input and outputs (with correlated noise)\")\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.scatter(x, data[:, i + 1], alpha=0.5)\n",
    "    ax.set_xlabel(\"Input\")\n",
    "    ax.set_ylabel(f\"Output {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"First few rows of the data:\")\n",
    "print(data[:5])\n",
    "\n",
    "# Plot correlation matrix of the outputs\n",
    "correlation_matrix = np.corrcoef(data[:, 1:].T)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Matrix of Outputs\")\n",
    "plt.xticks(range(3), [\"y1\", \"y2\", \"y3\"])\n",
    "plt.yticks(range(3), [\"y1\", \"y2\", \"y3\"])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, f\"{correlation_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating special multivariate LightGBM dataset\n",
    "\n",
    "Since LightGBM only supports a single target column, we need to create a special dataset for multivariate data.\n",
    "\n",
    "We do this by stacking the target columns on top of each other, and repeating the feature columns for each target column. This logic is contained in the `create_mv_dataset` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lightgbm dataset\n",
    "dtrain = create_mv_dataset(\n",
    "    data[:, 0].reshape(-1, 1),\n",
    "    data[:, 1:],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Selection\n",
    "\n",
    "We specify a Multivariate Gaussian distribution. By modifying the speciÔ¨Åcation in the following, the user can specify alternative distributional assumptions. This includes the option to choose from a wide range of parametric univariate distributions, as well as to model the data using Normalizing Flows. The user also has different function arguments for each distribution:\n",
    "\n",
    "- `stabilization`: specifies the stabilization method for the Gradient and Hessian. Options are `None`, `MAD` and `L2`.\n",
    "- `response_fn`: specifies $h_{k}(\\cdot)$ and transforms the distributional parameter to the correct support. Here, we specify an exponential for $\\sigma_{i}(\\cdot)$ only.\n",
    "- `loss_fn`: specifies the loss function used for training.  Options are `nll` (negative log-likelihood) or `crps` (continuous ranked probability score).\n",
    "\n",
    "For additional details, see `?MultivariateGaussian`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgblss = LightGBMLSS(\n",
    "    MultivariateGaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "Hyperparameter tuning and optimization is essential when fitting a model to all parameters of a Multivariate Gaussian at once. It's much more common for instabilities to arise with many interrelated outputs, meaning the default parameters are often not suitable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"eta\":                      [\"float\", {\"low\": 1e-5,   \"high\": 1,     \"log\": True}],\n",
    "    \"lambda_l1\":                [\"float\", {\"low\": 1e-5, \"high\": 100.0, \"log\": True}],\n",
    "    \"lambda_l2\":                [\"float\", {\"low\": 1e-5, \"high\": 100.0, \"log\": True}],\n",
    "    \"min_child_samples\":        [\"int\", {\"low\": 5, \"high\": 100, \"log\": False}],  # set to constant for this example\n",
    "    \"boosting\":                 [\"categorical\", [\"gbdt\"]],\n",
    "}\n",
    "\n",
    "np.random.seed(123)\n",
    "opt_param = lgblss.hyper_opt(param_dict,\n",
    "                             dtrain,\n",
    "                             num_boost_round=100,        # Number of boosting iterations.\n",
    "                             nfold=3,                    # Number of cv-folds.\n",
    "                             early_stopping_rounds=20,   # Number of early-stopping rounds\n",
    "                             max_minutes=60,             # Time budget in minutes, i.e., stop study after the given number of minutes.\n",
    "                             n_trials=30,               # The number of trials. If this argument is set to None, there is no limitation on the number of trials.\n",
    "                             silence=True,               # Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.\n",
    "                             seed=123,                   # Seed used to generate cv-folds.\n",
    "                             hp_seed=123                 # Seed for random number generator used in the Bayesian hyperparameter search.\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting results from all trials\n",
    "lgblss.study.trials_dataframe().sort_values(\"value\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "We use the optimized hyper-parameters and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "opt_params = opt_param.copy()\n",
    "n_rounds = opt_params[\"opt_rounds\"]\n",
    "del opt_params[\"opt_rounds\"]\n",
    "\n",
    "# Train Model with optimized hyperparameters\n",
    "lgblss.train(opt_params,\n",
    "             dtrain,\n",
    "             num_boost_round=n_rounds\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Similar to a LightGBM model, we now predict from the trained model. Different options are available:\n",
    "\n",
    "- `samples`: draws `n_samples` from the predicted distribution.\n",
    "- `quantiles`: calculates quantiles from the predicted distribution.\n",
    "- `parameters`: returns predicted distributional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Number of samples to draw from predicted distribution\n",
    "n_samples = 1000\n",
    "quant_sel = [0.05, 0.95] # Quantiles to calculate from predicted distribution\n",
    "\n",
    "# Make predictions\n",
    "preds = lgblss.predict(data[:, [0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "fig.suptitle(\"LightGBMLSS MV Gaussian Predictions\", fontsize=16)\n",
    "\n",
    "# Compute covariance matrices\n",
    "st = preds.iloc[:, 3:].to_numpy().reshape(-1, 3, 3, order=\"C\")\n",
    "cov = np.zeros_like(st)\n",
    "for i in range(st.shape[0]):\n",
    "    cov[i] = st[i] @ st[i].T\n",
    "\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    mean = preds[f\"loc_{i}\"]\n",
    "    std = np.sqrt(cov[:, i, i])\n",
    "    lower = mean - 1.96 * std\n",
    "    upper = mean + 1.96 * std\n",
    "    \n",
    "    ax.plot(data[:, [0]], mean, label=\"Mean\")\n",
    "    ax.fill_between(data[:, 0], lower, upper, alpha=0.3, label=\"95% CI\")\n",
    "    ax.scatter(data[:, 0], data[:, i+1], alpha=0.5, label=\"Data\")\n",
    "    ax.set_ylabel(f\"Y{i+1}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[-1].set_xlabel(\"Input\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
