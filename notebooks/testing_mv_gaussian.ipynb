{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. Add validation set for early stopping/eval\n",
    "2. Figure out way to get quicker convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate input data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "x = np.linspace(0, 10, n_samples)\n",
    "\n",
    "\n",
    "# Create correlated noise with increasing magnitude\n",
    "def scale_cov_matrix(x, base_cov, scale_factor=0.1):\n",
    "    return base_cov * (1 + scale_factor * x)\n",
    "\n",
    "\n",
    "cov_matrix = np.array([[0.25, 0.15, 0.1], [0.15, 0.49, 0.2], [0.1, 0.2, 0.36]])\n",
    "\n",
    "correlated_noise = np.array(\n",
    "    [\n",
    "        np.random.multivariate_normal(\n",
    "            mean=[0, 0, 0], cov=scale_cov_matrix(xi, cov_matrix)\n",
    "        )\n",
    "        for xi in x\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generate three dependent variables with smooth, non-monotonic relationships and correlated noise\n",
    "y1_ = 2 * np.sin(x) + 0.5 * x\n",
    "y1 = y1_ + correlated_noise[:, 0]\n",
    "y2_ = 3 * np.cos(0.5 * x) + 0.3 * x**2\n",
    "y2 = y2_ + correlated_noise[:, 1]\n",
    "y3_ = 1.5 * np.sin(0.7 * x) * np.cos(0.3 * x) + 0.2 * x\n",
    "y3 = y3_ + correlated_noise[:, 2]\n",
    "# Combine the data\n",
    "data = np.column_stack((x, y1, y2, y3))\n",
    "real_data = np.column_stack((x, y1_, y2_, y3_))\n",
    "\n",
    "# Plot the relationships\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "fig.suptitle(\"Relationships between input and outputs (with correlated noise)\")\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.scatter(x, data[:, i + 1], alpha=0.5)\n",
    "    ax.set_xlabel(\"Input\")\n",
    "    ax.set_ylabel(f\"Output {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"First few rows of the data:\")\n",
    "print(data[:5])\n",
    "\n",
    "# Plot correlation matrix of the outputs\n",
    "correlation_matrix = np.corrcoef(data[:, 1:].T)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(correlation_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Matrix of Outputs\")\n",
    "plt.xticks(range(3), [\"y1\", \"y2\", \"y3\"])\n",
    "plt.yticks(range(3), [\"y1\", \"y2\", \"y3\"])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, f\"{correlation_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbmlss.model import LightGBMLSS\n",
    "from lightgbmlss.utils import create_mv_dataset\n",
    "from lightgbmlss.distributions.Gaussian import MultivariateGaussian\n",
    "\n",
    "# Create lightgbm dataset\n",
    "dtrain = create_mv_dataset(\n",
    "    data[:, 0].reshape(-1, 1),\n",
    "    data[:, 1:],\n",
    ")\n",
    "\n",
    "# Create lightgbm lss model and train\n",
    "lgblss = LightGBMLSS(MultivariateGaussian(n_dim=3, response_fn=\"exp\"))\n",
    "lgblss.train(params={\"learning_rate\": 0.1, \"num_iterations\": 1000, \"lambda_l2\": 10, \"lambda_l1\": 10, \"num_leaves\": 2}, train_set=dtrain)\n",
    "\n",
    "# Make predictions\n",
    "preds = lgblss.predict(data[:, 0].reshape(-1, 1))\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbmlss.model import LightGBMLSS\n",
    "from lightgbmlss.utils import create_mv_dataset\n",
    "from lightgbmlss.distributions.Gaussian import MultivariateGaussian\n",
    "\n",
    "# Create lightgbm dataset\n",
    "dtrain = create_mv_dataset(\n",
    "    data[:, 0].reshape(-1, 1),\n",
    "    data[:, 1:],\n",
    ")\n",
    "\n",
    "param_dict = {\n",
    "    \"eta\": [\"float\", {\"low\": 1e-5, \"high\": 1, \"log\": True}],\n",
    "    \"max_depth\": [\"int\", {\"low\": 1, \"high\": 10, \"log\": False}],\n",
    "    \"num_leaves\": [\n",
    "        \"int\",\n",
    "        {\"low\": 255, \"high\": 255, \"log\": False},\n",
    "    ],  # set to constant for this example\n",
    "    \"min_data_in_leaf\": [\n",
    "        \"int\",\n",
    "        {\"low\": 20, \"high\": 20, \"log\": False},\n",
    "    ],  # set to constant for this example\n",
    "    \"min_gain_to_split\": [\"float\", {\"low\": 1e-8, \"high\": 40, \"log\": False}],\n",
    "    \"min_sum_hessian_in_leaf\": [\"float\", {\"low\": 1e-8, \"high\": 500, \"log\": True}],\n",
    "    \"subsample\": [\"float\", {\"low\": 0.2, \"high\": 1.0, \"log\": False}],\n",
    "    \"feature_fraction\": [\"float\", {\"low\": 0.2, \"high\": 1.0, \"log\": False}],\n",
    "    \"boosting\": [\"categorical\", [\"gbdt\"]],\n",
    "}\n",
    "\n",
    "# Create lightgbm lss model and train\n",
    "lgblss = LightGBMLSS(MultivariateGaussian(n_dim=3, response_fn=\"exp\"))\n",
    "opt_param = lgblss.hyper_opt(\n",
    "    param_dict,\n",
    "    dtrain,\n",
    "    num_boost_round=100,  # Number of boosting iterations.\n",
    "    nfold=5,  # Number of cv-folds.\n",
    "    early_stopping_rounds=20,  # Number of early-stopping rounds\n",
    "    max_minutes=10,  # Time budget in minutes, i.e., stop study after the given number of minutes.\n",
    "    n_trials=30,  # The number of trials. If this argument is set to None, there is no limitation on the number of trials.\n",
    "    silence=True,  # Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.\n",
    "    seed=123,  # Seed used to generate cv-folds.\n",
    "    hp_seed=123,  # Seed for random number generator used in the Bayesian hyperparameter search.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbmlss.model import LightGBMLSS\n",
    "from lightgbmlss.utils import create_mv_dataset\n",
    "from lightgbmlss.distributions.Gaussian import MultivariateGaussian\n",
    "\n",
    "# Create lightgbm dataset\n",
    "dtrain = create_mv_dataset(\n",
    "    data[:, 0].reshape(-1, 1),\n",
    "    data[:, 1:],\n",
    ")\n",
    "\n",
    "param_dict = {\n",
    "    \"lambda_l1\": [\"float\", {\"low\": 1e-8, \"high\": 10.0, \"log\": True}],\n",
    "    \"lambda_l2\": [\"float\", {\"low\": 1e-8, \"high\": 100.0, \"log\": True}],\n",
    "    \"num_leaves\": [\n",
    "        \"int\",\n",
    "        {\"low\": 2, \"high\": 255, \"log\": False},\n",
    "    ],  # set to constant for this example\n",
    "    \"min_child_samples\": [\"int\", {\"low\": 5, \"high\": 100, \"log\": False}],\n",
    "    \"eta\": [\"float\", {\"low\": 1e-5, \"high\": 1, \"log\": True}],\n",
    "    # \"num_iterations\": [\"int\", {\"low\": 100, \"high\": 1000, \"log\": False}],\n",
    "    # \"feature_fraction\": [\"float\", {\"low\": 0.4, \"high\": 1.0, \"log\": False}],\n",
    "    # \"bagging_fraction\": [\"float\", {\"low\": 0.4, \"high\": 1.0, \"log\": False}],\n",
    "    # \"bagging_freq\": [\"int\", {\"low\": 1, \"high\": 7, \"log\": False}],\n",
    "    \"feature_pre_filter\": [\"bool\", False],\n",
    "}\n",
    "\n",
    "# Create lightgbm lss model and train\n",
    "lgblss = LightGBMLSS(MultivariateGaussian(n_dim=3, response_fn=\"exp\"))\n",
    "opt_param = lgblss.hyper_opt(\n",
    "    param_dict,\n",
    "    dtrain,\n",
    "    num_boost_round=100,  # Number of boosting iterations.\n",
    "    nfold=5,  # Number of cv-folds.\n",
    "    early_stopping_rounds=20,  # Number of early-stopping rounds\n",
    "    max_minutes=10000,  # Time budget in minutes, i.e., stop study after the given number of minutes.\n",
    "    n_trials=30,  # The number of trials. If this argument is set to None, there is no limitation on the number of trials.\n",
    "    silence=True,  # Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.\n",
    "    seed=123,  # Seed used to generate cv-folds.\n",
    "    hp_seed=123,  # Seed for random number generator used in the Bayesian hyperparameter search.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbmlss.model import LightGBMLSS\n",
    "from lightgbmlss.utils import create_mv_dataset\n",
    "from lightgbmlss.distributions.Gaussian import MultivariateGaussian\n",
    "\n",
    "# Create lightgbm dataset\n",
    "dtrain = create_mv_dataset(\n",
    "    data[:, 0].reshape(-1, 1),\n",
    "    data[:, 1:],\n",
    ")\n",
    "\n",
    "# Create lightgbm lss model and train\n",
    "lgblss = LightGBMLSS(MultivariateGaussian(n_dim=3, response_fn=\"exp\"))\n",
    "lgblss.train(params={**opt_param, \"num_iterations\": 1000}, train_set=dtrain)\n",
    "\n",
    "# Make predictions\n",
    "preds = lgblss.predict(data[:, 0].reshape(-1, 1))\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "dist = MultivariateNormal(\n",
    "    loc=torch.tensor(preds.iloc[:, :3].to_numpy()),\n",
    "    scale_tril=torch.tensor(preds.iloc[:, 3:].to_numpy().reshape(-1, 3, 3, order=\"C\")),\n",
    ")\n",
    "loss = -torch.nansum(dist.log_prob(torch.tensor(data[:, 1:])))\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dist = MultivariateNormal(\n",
    "    loc=torch.tensor(real_data[:, 1:]),\n",
    "    covariance_matrix=torch.tensor(np.tile(cov_matrix, (n_samples, 1, 1))),\n",
    ")\n",
    "loss = -torch.nansum(dist.log_prob(torch.tensor(data[:, 1:])))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(params):\n",
    "\n",
    "    # Create lightgbm lss model and train\n",
    "    lgblss = LightGBMLSS(MultivariateGaussian(n_dim=3))\n",
    "    lgblss.train(params=params, train_set=dtrain)\n",
    "    preds = lgblss.predict(data[:, 0].reshape(-1, 1))\n",
    "    dist = MultivariateNormal(\n",
    "        loc=torch.tensor(preds.iloc[:, :3].to_numpy()),\n",
    "        scale_tril=torch.tensor(preds.iloc[:, 3:].to_numpy().reshape(-1, 3, 3, order=\"C\")),\n",
    "    )\n",
    "    loss = -torch.nansum(dist.log_prob(torch.tensor(data[:, 1:])))\n",
    "    return loss\n",
    "# print(get_loss({\"learning_rate\": 1e-10, \"num_iterations\": 1}))\n",
    "# print(get_loss({\"learning_rate\": 0.01, \"num_iterations\": 10}))\n",
    "# print(get_loss({\"learning_rate\": 0.01, \"num_iterations\": 20}))\n",
    "# print(get_loss({\"learning_rate\": 0.01, \"num_iterations\": 50}))\n",
    "# print(get_loss({\"learning_rate\": 0.01, \"num_iterations\": 100}))\n",
    "# print(get_loss({\"learning_rate\": 0.1, \"num_iterations\": 1000}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "fig.suptitle(\"LightGBMLSS MV Gaussian Predictions\", fontsize=16)\n",
    "\n",
    "# Compute covariance matrices\n",
    "st = preds.iloc[:, 3:].to_numpy().reshape(-1, 3, 3, order=\"C\")\n",
    "cov = np.zeros_like(st)\n",
    "for i in range(st.shape[0]):\n",
    "    cov[i] = st[i] @ st[i].T\n",
    "\n",
    "for i in range(3):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    mean = preds[f\"loc_{i}\"]\n",
    "    std = np.sqrt(cov[:, i, i])\n",
    "    lower = mean - 1.96 * std\n",
    "    upper = mean + 1.96 * std\n",
    "    \n",
    "    ax.plot(data[:, [0]], mean, label=\"Mean\")\n",
    "    ax.fill_between(data[:, 0], lower, upper, alpha=0.3, label=\"95% CI\")\n",
    "    ax.scatter(data[:, 0], data[:, i+1], alpha=0.5, label=\"Data\")\n",
    "    ax.set_ylabel(f\"Y{i+1}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[-1].set_xlabel(\"Input\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[999, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and Hessian analysis\n",
    "\n",
    "To get the data analyzed below, I saved the gradient and hessian values for every point, iteration, parameter combination through a training of the model\n",
    "\n",
    "\n",
    "For more details on how lightgbm calculates gain from each split using gradients and hessians:\n",
    "\n",
    "* [XGBoost paper](https://arxiv.org/pdf/1603.02754)\n",
    "* [Code line where gain is calculated](https://github.com/microsoft/LightGBM/blob/d2b4e7374957e0d05a3a7d5ec695940287d4dc36/src/treelearner/feature_histogram.hpp#L808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "grad = pd.read_csv(\"grad_hess5.csv\")\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.pivot_table(index=\"iter\", columns=\"cat\", values=\"00\", aggfunc=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pts = range(960, 1000)\n",
    "tmp = grad.query(\"pt.isin(@pts) & iter < 1 & iter > -1\")\n",
    "fig, axes = plt.subplots(figsize=(12, 12), ncols=3, nrows=3, sharex=True, sharey=True)\n",
    "\n",
    "for i, col in enumerate(col for col in tmp.columns if col not in [\"pt\", \"iter\", \"cat\"]):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    curr = tmp.pivot_table(index=\"iter\", columns=\"cat\", values=col, aggfunc=\"sum\")\n",
    "    curr[\"ratio\"] = curr[\"grad\"] ** 2 / (curr[\"hess\"] + 1e-15) #* np.sign(curr[\"grad\"])\n",
    "    curr.plot(ax=ax, marker=\"o\")\n",
    "    ax.set_title(col)\n",
    "    ax.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pts = range(960, 1000)\n",
    "tmp = grad.query(\"pt.isin(@pts) & iter < 3 & iter > -1\")\n",
    "fig, axes = plt.subplots(figsize=(12, 12), ncols=3, nrows=3, sharex=True, sharey=False)\n",
    "\n",
    "for i, col in enumerate(col for col in tmp.columns if col not in [\"pt\", \"iter\", \"cat\"]):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    curr = tmp.pivot_table(index=\"iter\", columns=\"cat\", values=col, aggfunc=\"sum\")\n",
    "    curr[\"ratio\"] = curr[\"grad\"] ** 2 / (curr[\"hess\"] + 1e-15) * np.sign(curr[\"grad\"])\n",
    "    curr[[\"ratio\"]].plot(ax=ax, marker=\"o\")\n",
    "    ax.set_title(col)\n",
    "    # ax.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building torch distribution gradient intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal, MultivariateNormal\n",
    "import torch\n",
    "from torch.autograd import grad as autograd\n",
    "\n",
    "\n",
    "loc = torch.tensor([1.], requires_grad=True)\n",
    "scale = torch.tensor([1.], requires_grad=True)\n",
    "target = torch.tensor([-5.])\n",
    "loss = -torch.nansum(Normal(loc=loc, scale=scale).log_prob(target))\n",
    "\n",
    "print(f\"Point: {target.item():.2f}\\nMean: {loc.item():.2f}, STD: {scale.item():.2f}\\n\")\n",
    "\n",
    "grad_loc = autograd(loss, inputs=loc, create_graph=True)\n",
    "hess_loc = autograd(grad_loc[0].nansum(), inputs=loc, retain_graph=True)\n",
    "print(f\"Mean Gradient: {grad_loc[0].item():.2f}\")\n",
    "print(f\"Mean Hessian: {hess_loc[0].item():.2f}\\n\")\n",
    "grad_scale = autograd(loss, inputs=scale, create_graph=True)\n",
    "hess_scale = autograd(grad_scale[0].nansum(), inputs=scale, retain_graph=True)\n",
    "print(f\"STD Gradient: {grad_scale[0].item():.2f}\")\n",
    "print(f\"STD Hessian: {hess_scale[0].item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "35**2/107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal, MultivariateNormal\n",
    "import torch\n",
    "from torch.autograd import grad as autograd\n",
    "\n",
    "\n",
    "loc = torch.tensor([2.87, 9.38, 1.42], requires_grad=True)\n",
    "scale_tril = torch.tensor([[2.03, 0, 0], [6.25, 5.51, 0], [0.53, -0.15, 0.70]], requires_grad=True)\n",
    "target = torch.tensor([4.41, 32.12, 1.65])\n",
    "dist = MultivariateNormal(loc=loc, scale_tril=scale_tril)\n",
    "loss = -torch.nansum(dist.log_prob(target))\n",
    "\n",
    "print(f\"Actual: {target}\\n\\nMean: {loc}\\nscale_tril: {scale_tril}\\n\")\n",
    "print(f\"True STD:\\n{np.sqrt((scale_tril @ scale_tril.T).detach().numpy())}\\n\")\n",
    "\n",
    "grad_loc = autograd(loss, inputs=loc, create_graph=True)\n",
    "hess_loc = autograd(grad_loc[0].nansum(), inputs=loc, retain_graph=True)\n",
    "print(f\"Mean Gradient: {grad_loc[0]}\")\n",
    "print(f\"Mean Hessian: {hess_loc[0]}\\n\")\n",
    "grad_scale_tril = autograd(loss, inputs=scale_tril, create_graph=True)\n",
    "hess_scale_tril = autograd(grad_scale_tril[0].nansum(), inputs=scale_tril, retain_graph=True)\n",
    "print(f\"STD Gradient: {grad_scale_tril[0]}\")\n",
    "print(f\"STD Hessian: {hess_scale_tril[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_tril @ scale_tril.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.covariance_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
